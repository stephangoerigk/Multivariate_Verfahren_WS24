---
title: "Explorative Faktorenanalyse"
output: 
  html_document:
    number_sections: false
    toc: true
    toc_depth: 6
---

<style>
body {
  font-family: Arial, sans-serif;
  font-size: 12px;  
  line-height: 1.6;
  margin: 20px;
  color: #333;
}

h1.title {
  font-size: 20px;  
  color: #444;
  font-weight: bold;
}

h1, h2, h3 {
  font-size: 14px;  
  color: #444;
  font-weight: bold;
}

h4, h5, h6, h7, h8 {
  font-size: 14px;  
  color: #444;
  font-weight: bold;
}

a {
  color: #0066cc;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

pre, code {
  font-size: 11px;  
  background-color: #f7f7f7;  
  padding: 8px;  
  border: 1px solid #ddd;  
  border-radius: 4px;  
  overflow-x: auto;  
}

img {
  max-width: 70%; 
  height: auto; 
}

img[src*="fig-"], 
img[class*="figure"] {
  max-width: 70% !important;
  max-height: 70% !important;
  height: auto !important;
}

</style>



Die explorative Faktorenanalyse (EFA) ist eine statistische Methode, die dazu dient, **Strukturen in einem Datensatz zu entdecken** und Gruppen von Variablen zu identifizieren, die gemeinsam variieren. Die EFA reduziert eine große Anzahl von Variablen auf eine kleinere Anzahl zugrunde liegender Dimensionen (**Faktoren**), was die Komplexität der Daten verringert und wesentliche Muster offenlegt. Dadurch können theoretische Modelle entwickelt oder überprüft werden (Abgrenzung zur **konfirmatorischen Faktorenanalyse**). Ein weiteres Ziel der EFA ist es, die Dimensionalität und Konstruktvalidität von Messinstrumenten zu überprüfen, um sicherzustellen, dass diese die postulierten Dimensionen valide erfassen.

## Zwei "Unterfamilien" faktorenanalytischer Methoden

- **Konfirmatorische Faktorenanalyse (CFA):** Eine **hypothesenprüfende** Methode, bei der ein vorab festgelegtes Modell getestet wird, das die Zuordnung von Indikatoren zu latenten Variablen definiert. Ziel der CFA ist es, die Passung des Modells zu den beobachteten Daten zu überprüfen.

- **Explorative Faktorenanalyse (EFA):** Eine **hypothesengenerierende** Methode, die verwendet wird, wenn die Zuordnung der Indikatoren zu den latenten Variablen noch unbekannt ist. Ziel der EFA ist es, verborgene Faktorenstrukturen in den Daten zu entdecken.

Beide Methoden zielen darauf ab, wechselseitig unabhängige Faktoren zu identifizieren, die die Zusammenhänge zwischen den Variablen erklären.

## Grundlegendes

- **Korrelation manifester Variablen:** Die EFA basiert auf der Beobachtung, dass Korrelationen zwischen manifesten (direkt gemessenen) Variablen auf zugrunde liegende Strukturen hinweisen.

- **Erklärung der Zusammenhänge:** Die zentrale Idee der EFA ist, dass die beobachteten Zusammenhänge durch latente Variablen erklärt werden, die die Variationen in den manifesten Variablen verursachen.

## Implikationen der Faktorenanalyse

- **Manifeste Variablen als Indikatoren:** Manifeste Variablen dienen als Indikatoren für latente Variablen, wodurch Variationen in den gemessenen Variablen durch diese zugrunde liegenden Faktoren erklärt werden.

- **Unkorrelierte Messwerte nach Kontrolle der latenten Variablen:** Nach Berücksichtigung der latenten Variablen sollten die verbleibenden Messwerte der manifesten Variablen unkorreliert sein, da die latenten Variablen die wesentlichen Zusammenhänge erklären.

---

## Beispiel zur Anwendung der Explorativen Faktorenanalyse (EFA)

Um die Anwendung der EFA zu verdeutlichen, betrachten wir ein Beispiel mit simulierten Daten des NEO-Fünf-Faktoren-Inventars (NEO-FFI). Das NEO-FFI ist ein weit verbreitetes psychometrisches Instrument zur Messung der fünf grundlegenden Dimensionen der Persönlichkeit: **Neurotizismus, Extraversion, Offenheit für Erfahrungen, Verträglichkeit** und **Gewissenhaftigkeit**.

In diesem Beispiel sollen die simulierten, bereits korrekt gepolten Daten explorativ untersucht werden, um zu überprüfen, ob die Daten die erwartete Fünf-Faktoren-Struktur wiederspiegeln. Das Ziel der EFA in diesem Kontext ist es, festzustellen, ob die Dimensionen des NEO-FFI durch die zugrunde liegenden Faktoren in den simulierten Daten korrekt repräsentiert werden.

<font color='darkred'>Hinweis: In unserem Beispiel könnte man argumentieren, dass die CFA eine geeignetere Methode wäre, da die Fünf-Faktoren-Struktur gut etabliert erscheint. Die CFA würde dabei helfen, die Hypothese zu überprüfen, dass genau diese fünf Faktoren den Daten zugrunde liegen.

Wir wollen in diesem Fall jedoch davon ausgehen, dass die Struktur der Daten noch nicht vollständig bekannt ist. **Stellen Sie sich vor, wir stehen noch vor der Entdeckung dieser Struktur:** Wir haben eine Sammlung von Variablen, die möglicherweise durch zugrunde liegende Faktoren erklärt werden können, kennen jedoch weder die Anzahl noch die Natur dieser Faktoren. In einer solchen Situation ist die EFA die Methode der Wahl.</font>

```{r DataSet, include = FALSE}

library(readr)
NEO = read_csv("~/Desktop/NEO_FFI_original.csv")
nrow(NEO)

NEO[] = lapply(NEO, function(x) {
  if (is.factor(x)) as.numeric(as.character(x)) else x
})

NEO[] = lapply(NEO, function(x) {
  if (is.character(x)) as.numeric(x) else x
})

options(scipen=999)

NEO = subset(NEO, select = -c(sex, age, neurotiz, extraver, offenhei, verträgl, gewissen)) 

# set.seed(123)  
# 
# NEO = as.data.frame(lapply(NEO, function(x) {
#   if (is.numeric(x)) {
#     rnorm(n = length(x), mean = 0, sd = 0.1)
#   } else {
#     x
#   }
# }))
# 
# if (!require("openxlsx")) {
#   install.packages("openxlsx")
# }
# library(openxlsx)
# 
# write.xlsx(NEO, "NEO.xlsx")

```

---

### Setup
```{r Setup, message=FALSE, warning=FALSE}

# install.packages("psych")
library(psych)

```

---

### Prüfung der Anwendungsvoraussetzungen für eine Explorative Faktorenanalyse
Bevor die EFA durchgeführt wird, sollten die Daten auf ihre Eignung geprüft werden. Zu den wichtigsten Voraussetzungen zählen:

- **Korrelation zwischen den Variablen:** Die im Fokus stehenden Variablen müssen ausreichend miteinander korrelieren, damit latente Faktoren identifiziert werden können. Dies wird durch den Bartlett-Test überprüft.
- **Kaiser-Meyer-Olkin (KMO) Test:** Der KMO-Test bewertet die Eignung der Daten für die EFA. Werte über 0,5 gelten als akzeptabel, über 0,8 als gut.

Darüber hinaus sollten folgende Punkte berücksichtigt werden:
- **Stichprobengröße:** Eine allgemeine Faustregel besagt, dass pro Variable mindestens 5 bis 10 Beobachtungen vorliegen sollten, um stabile Ergebnisse zu gewährleisten. Bei komplexen Modellen oder schwachen Korrelationen kann eine größere Stichprobe erforderlich sein (mindestens 300 Beobachtungen).
- **Multikollinearität:** Zu starke Korrelationen zwischen den Variablen (Multikollinearität) können die Analyse verzerren. Eine Möglichkeit zur Überprüfung ist die Determinante der Korrelationsmatrix. Eine Determinante nahe 0 deutet auf problematische Multikollinearität hin und sollte vermieden werden.

```{r Voraussetzungen EFA, message=FALSE, warning=FALSE}

# Bartlett-Test
cor_matrix_NEO = cor(NEO)
cortest.bartlett(cor_matrix_NEO, n = nrow(NEO), diag = TRUE)

# KMO Test
KMO(NEO)

# Determinante der Korrelationsmatrix
det(cor_matrix_NEO)

```

Die Ergebnisse zeigen, dass die Daten für die EFA geeignet sind. Der Bartlett-Test ist signifikant, was darauf hinweist, dass zwischen den Variablen ausreichende Korrelationen bestehen. Der KMO Wert liegt über der kritischen Schwelle von 0.50. Einige Variablen weisen MSA-Werte unter 0,5 auf, was auf eine geringere Eignung dieser Items hinweist. Insgesamt ist die Durchführung einer EFA jedoch gerechtfertigt, auch wenn schwächere Variablen möglicherweise überdacht oder entfernt werden sollten.

---

### 1. Auswahl und Begründung der Methode der Faktorenextraktion
Nachdem wir die Voraussetzungen für die EFA überprüft haben und festgestellt wurde, dass die Daten grundsätzlich geeignet sind, geht es nun um die Auswahl der richtigen Methode zur Faktorenextraktion. Diese Wahl hängt von der Zielsetzung der Analyse ab. In der EFA werden häufig die Hauptkomponentenanalyse (engl. Principal Component Analysis, PCA) und die Hauptachsenanalyse (engl. Principal Factor Analysis, PFA) verwendet.

- Die PFA fokussiert sich auf die gemeinsame Varianz zwischen den Variablen und wird häufig verwendet, wenn das Ziel darin besteht, latente Faktoren zu identifizieren, die den beobachteten Variablen zugrunde liegen. Da sie die spezifische Varianz und Fehleranteile nicht berücksichtigt, ist sie besonders nützlich, wenn man an den zugrunde liegenden Strukturen interessiert ist, die die Variablen erklären. Dies ist in unserem Fall besonders relevant, da wir die latenten Persönlichkeitsfaktoren der NEO-Daten identifizieren wollen.
- Im Gegensatz dazu berücksichtigt die PCA sowohl die gemeinsame als auch die spezifische Varianz und wird häufig verwendet, wenn man die gesamte Varianz der Daten erklären möchte. Sie eignet sich gut zur Datenreduktion, wenn die Zielsetzung eher explorativ ist.

In dieser Analyse liegt der Fokus darauf, die zugrunde liegenden latenten Strukturen zu entdecken, die die Variablen des NEO-Tests beeinflussen. Daher ist die PFA die geeignetere Methode, da sie spezifisch darauf abzielt, nur die gemeinsame Varianz zu erklären und latente Faktoren zu extrahieren.

Zum aktuellen Zeitpunkt ist es nicht notwendig, einen spezifischen Code anzugeben, da die Faktorenextraktion in den späteren Schritten der Analyse definiert wird. Die Implementierung der PFA erfolgt im entsprechenden Codeabschnitt, sobald die praktische Analyse beginnt.

---

### 2. Faktorenanzahl bestimmen
Ein entscheidender Schritt in der EFA ist die Bestimmung der optimalen Anzahl der zu extrahierenden Faktoren. Diese Entscheidung hängt von mehreren Kriterien ab, die sicherstellen sollen, dass nur die wesentlichen latenten Strukturen erfasst werden, ohne unnötig viele Faktoren zu extrahieren.

Zu den gängigsten Methoden zur Bestimmung der Faktorenanzahl gehören:

- **Kaiser-Kriterium**: Nach dem Kaiser-Kriterium werden nur die Faktoren beibehalten, deren Eigenwerte größer als 1 sind. Ein Eigenwert über 1 bedeutet, dass der Faktor mehr Varianz erklärt, als eine einzelne Variable erklären würde. Diese Methode ist einfach anzuwenden, aber sie kann dazu neigen, zu viele Faktoren zu identifizieren.

- **Scree-Test**: Der Scree-Test visualisiert die Eigenwerte in einer absteigenden Reihenfolge in einem sogenannten "Scree-Plot". Man sucht nach einem "Knick" in der Kurve, ab dem die Eigenwerte nur noch geringfügig sinken. Die Anzahl der Faktoren wird dann bis zu diesem Knick festgelegt. Diese Methode wird häufig verwendet, da sie die Anzahl der relevanten Faktoren grafisch verdeutlicht.

- **Parallelanalyse**: Diese Methode vergleicht die Eigenwerte der realen Daten mit zufällig generierten Daten. Nur die Faktoren, deren Eigenwerte die der Zufallsdaten übersteigen, werden als signifikant betrachtet. Die Parallelanalyse gilt als eine der zuverlässigsten Methoden zur Bestimmung der Faktorenanzahl.

- **Interpretierbarkeit**: Letztlich sollte die Anzahl der Faktoren auch anhand ihrer inhaltlichen und theoretischen Interpretierbarkeit festgelegt werden. Selbst wenn ein statistisches Kriterium eine bestimmte Anzahl von Faktoren vorschlägt, sollte geprüft werden, ob diese Faktoren sinnvoll interpretiert werden können.

In dieser Analyse verwenden wir eine Kombination aus dem Scree-Test und der Parallelanalyse, um die optimale Anzahl an Faktoren zu bestimmen. Dies stellt sicher, dass sowohl statistische Kriterien als auch die Interpretierbarkeit berücksichtigt werden.

```{r Faktorenanzahl bestimmen, message=FALSE, warning=FALSE}

# Kaiser-Kriterium: Eigenwerte über 1 extrahieren
eigenvalues = eigen(cor(NEO))$values
sum(eigenvalues > 1)

# Scree-Test: Scree-Plot 
scree(NEO)

# Parallelanalyse: Vergleich der Eigenwerte mit Zufallsdaten
fa.parallel(NEO, fa = "fa", fm = "ml", n.iter = 1000, quant = 0.95, plot = TRUE)

```

Die Ergebnisse der Scree-Plot- und Parallelanalyse zeigen, dass 5 Faktoren die zugrunde liegenden Strukturen der Daten am besten erklären. Der Scree-Plot weist auf einen deutlichen Knick nach dem fünften Faktor hin, was darauf hindeutet, dass zusätzliche Faktoren keinen signifikanten Beitrag zur Erklärung der Varianz leisten. Die Parallelanalyse bestätigt diese Interpretation, da die Eigenwerte der tatsächlichen Daten bis zum fünften Faktor über den zufällig generierten Eigenwerten liegen. Dies bedeutet, dass die fünf extrahierten Faktoren die wesentlichen latenten Strukturen der Daten repräsentieren.

---

#### Minimal Average Partial (MAP)-Test und Bayesian Information Criterion (BIC) mit Maximum Likelihood (ML) Methode
Der **Minimal Average Partial (MAP)-Test** ist eine Methode zur Bestimmung der optimalen Faktorenanzahl in der EFA. Im Gegensatz zur Parallelanalyse, die auf den Eigenwerten basiert, fokussiert sich der MAP-Test auf die Minimierung der durchschnittlichen partiellen Korrelationen zwischen den Variablen nach der Extraktion von Faktoren. Ziel des Tests ist es, die Anzahl der Faktoren zu bestimmen, bei der die verbleibenden Korrelationen am geringsten sind. Dies deutet darauf hin, dass diese Anzahl von Faktoren die zugrunde liegende Struktur der Daten am besten beschreibt.

Das **Bayesian Information Criterion (BIC)** ist ein Modellvergleichsmaß, das zur Auswahl der optimalen Anzahl von Faktoren verwendet wird. Das BIC berücksichtigt die Modellanpassung und die Anzahl der Parameter (d.h. Faktoren), um das beste Modell zu bestimmen. Ein niedrigerer BIC-Wert deutet auf eine bessere Modellanpassung hin. Da BIC die Modellkomplexität berücksichtigt, bestraft es Modelle mit zu vielen Faktoren stärker, um Überanpassung zu vermeiden.

Im Code wird die **Maximum-Likelihood (ML)-Methode** zur Faktorextraktion verwendet (`fm = "ml"`). Die **ML-Methode** schätzt die Faktoren so, dass die Korrelationen zwischen den Variablen am besten durch die latenten Faktoren erklärt werden. Dies ermöglicht präzisere Schätzungen und eine bessere Bewertung der Modellanpassung durch statistische Tests.

```{r MAP und BIC, message=FALSE, warning=FALSE}

VSS(NEO, rotate = "oblimin", fm = "ml", cor = "cor")

```

In diesem Code wird die **Vielschichtigkeits-Analyse (VSS)** durchgeführt, um die Anzahl der zu extrahierenden Faktoren in einer Faktorenanalyse zu bestimmen. Die **VSS-Analyse** hilft dabei, die beste Anzahl von Faktoren zu ermitteln, die die Varianz in den Daten erklären. Die Berechnungen basieren auf der **Korrelationsmatrix** der Daten (`cor = "cor"`), was in der psychologischen Forschung üblich ist, da viele Tests auf den Interkorrelationen der Items beruhen.

Im Code wird die **Maximum-Likelihood (ML)-Methode** zur Faktorextraktion verwendet (`fm = "ml"`). Die **ML-Methode** schätzt die Faktoren so, dass die Korrelationen zwischen den Variablen am besten durch die latenten Faktoren erklärt werden. Diese Methode ermöglicht präzisere Schätzungen und eine bessere Bewertung der Modellanpassung durch statistische Tests. Die Entscheidung, die ML-Methode zu verwenden, kann notwendig sein, um die Qualität der Schätzung zu verbessern und Modellanpassungen zu bewerten.

Die Parameter im Code sind wie folgt:

- **`rotate = "oblimin"`**: Dies gibt an, dass eine **oblique Rotation** (nicht orthogonal) durchgeführt wird. Die **Oblimin-Rotation** erlaubt es den Faktoren, miteinander korreliert zu sein, was häufig zu realistischeren und interpretierbaren Ergebnissen führt.

**Zusammenfassung:** Die VSS-Analyse hilft bei der Entscheidung, wie viele Faktoren extrahiert werden sollten, um die Daten angemessen zu beschreiben. Die Verwendung der ML-Methode zur Faktorextraktion ermöglicht eine genauere Modellanpassung und -bewertung. Die oblique Rotation sorgt für interpretierbare Ergebnisse, indem sie die Korrelationen zwischen den Faktoren berücksichtigt.

---

#### Empirical Kaiser Criterion (EKC)

Das **Empirical Kaiser Criterion (EKC)** basiert auf dem **Kaiser-Guttman-Kriterium**, das eine häufig verwendete Methode zur Bestimmung der Anzahl von Faktoren in einer explorativen Faktorenanalyse ist. Es filtert Faktoren mit **Eigenwerten größer als 1** heraus. Ein Eigenwert gibt an, wie viel Varianz in den Daten durch einen bestimmten Faktor erklärt wird; ein Wert über 1 deutet darauf hin, dass der Faktor mehr Varianz erklärt als ein einzelnes Item.

```{r Faktorenanalyse Empirical Kaiser Criterion (EKC), warning=FALSE, message=FALSE, error=FALSE}

n = nrow(NEO) # Anzahl der Beobachtungen (Zeilen) im Datensatz
p = ncol(NEO) # Anzahl der Variablen (Spalten) im Datensatz

dat_cor = cor(NEO) # Korrelationsmatrix des Datensatzes wird berechnet

eigval = eigen(dat_cor)$values # Eigenwerte der Korrelationsmatrix werden berechnet und in "eigval" gespeichert

comp = rep(0, p)# Vektor "comp" wird erstellt, um die Vergleichswerte für die Eigenwerte zu speichern

for (j in 1:p) {
  comp[j] = max(((1 + sqrt(p/n))^2) * (p - sum(comp[1:j-1])) / (p-j+1), 1)
}

```

In diesem Code wird das **Empirical Kaiser Criterion (EKC)** zur Bestimmung der optimalen Anzahl von Faktoren berechnet.

Die **Anzahl der Beobachtungen (`n`)** wird durch die Anzahl der Zeilen im Datensatz `NEO` ermittelt. Dies repräsentiert die Anzahl der Beobachtungen.
Die **Anzahl der Variablen (`p`)** wird durch die Anzahl der Spalten im Datensatz berechnet. Dies gibt die Anzahl der Variablen an.
Die **Korrelationsmatrix (`dat_cor`)** der Daten wird berechnet, um die Zusammenhänge zwischen den Variablen zu erfassen.
Die **Eigenwerte (`eigval`)** der Korrelationsmatrix werden berechnet, die den Varianzanteil erklären, der durch die Faktoren beschrieben wird.
Ein **Vektor für Vergleichswerte (`comp`)** wird erstellt. Die Vergleichswerte werden anhand einer Formel berechnet, die die Anzahl der Variablen, Beobachtungen und bereits berechneten Werte berücksichtigt.

Dieser Prozess ermöglicht es, die Eigenwerte der empirischen Daten mit den Vergleichswerten zu vergleichen, um zu bestimmen, welche Anzahl von Faktoren signifikant ist und somit die zugrunde liegende Struktur der Daten beschreibt.

---

#### Oblimin Rotation ohne Kaiser-Normalisierung
Die **Oblimin-Rotation** ist eine Methode zur Rotation von Faktoren in der Faktorenanalyse, die eine oblique (schiefe) Rotation ermöglicht. Dies bedeutet, dass die Faktoren korreliert sein können, was oft eine realistischere Darstellung der Daten liefert. Die Rotation hilft dabei, eine klarere und interpretierbare Struktur der Faktoren zu erhalten.

In diesem Beispiel wird die **Oblimin-Rotation** ohne Kaiser-Normalisierung angewendet. Die Kaiser-Normalisierung, die üblicherweise verwendet wird, um die Skalen der Variablen zu standardisieren, wird hier nicht berücksichtigt.

```{r Oblimin Rotation ohne Kaiser-Normalisierung, message=FALSE, warning=FALSE}

fit1 = fa(NEO, nfactors = 5, rotate = "oblimin", residuals = TRUE, SMC = TRUE, cor = "cor", n.iter = 1000, p = 0.05)

# fa()-Funktion aus dem "psych"-Paket führt eine Faktorenanalyse durch und rotiert die Faktoren mit der Oblimin-Methode. 

fa.sort(fit1)

```

In diesem Code wird die **Faktorenanalyse** (`fa()`) mit der **Oblimin-Rotation** durchgeführt, wobei die Kaiser-Normalisierung nicht berücksichtigt wird.

Die **Anzahl der zu extrahierenden Faktoren (`nfactors = 5`)** gibt an, dass fünf Faktoren extrahiert werden sollen.
Die **Oblimin-Rotation (`rotate = "oblimin"`)** wird verwendet, eine Methode der obliquen Rotation, die es den Faktoren ermöglicht, miteinander korreliert zu sein.
Mit **`residuals = TRUE`** werden die Residuen der Modellanpassung berechnet, um die Unterschiede zwischen den beobachteten und geschätzten Korrelationsmatrizen zu überprüfen.
Die **quadratischen multiplen Korrelationen (`SMC = TRUE`)** werden zur Schätzung der Kommunalitäten verwendet, um die Menge der Varianz, die durch die Faktoren erklärt wird, besser abzuschätzen.
Die **Korrelationsmatrix (`cor = "cor"`)** der Daten wird genutzt.
# Die **Maximum-Likelihood-Methode (`fm = "ml"`)** wird zur Faktorextraktion verwendet, um die beobachteten Korrelationen durch die extrahierten Faktoren bestmöglich zu erklären.
Mit **`n.iter = 2000`** wird die Anzahl der Iterationen auf 2000 gesetzt, um sicherzustellen, dass das Modell ausreichend konvergiert.
Das **Signifikanzniveau (`p = 0.05`)** für die Tests wird festgelegt.

Nach der Durchführung der Faktoranalyse wird `fa.sort(fit1)` verwendet, um die Ergebnisse der Rotation zu sortieren und die Interpretation der Faktoren und ihrer Ladungen zu erleichtern.

---

#### Oblimin-Rotation mit Kaiser-Normalisierung

In diesem Code wird die **Faktorenanalyse** (`fa()`) ohne Rotation initial durchgeführt und anschließend mit der **Kaiser-Normalisierung** und **Oblimin-Rotation** angepasst.

```{r Oblimin-Rotation mit Kaiser-Normalisierung, message=FALSE, warning=FALSE}

fit2 = fa(NEO, nfactors = 5, rotate = "none", residuals = TRUE, SMC = TRUE)
fit3 = kaiser(fit2, rotate = "oblimin")
fa.sort(fit3)

```

In diesem Code wird eine **Faktorenanalyse** (`fa()`) durchgeführt, bei der zunächst keine Rotation angewendet wird, da **`rotate = "none"`** angegeben ist. # Die **Maximum-Likelihood-Methode (ML)** zur Faktorextraktion wird verwendet, wie durch **`fm = "ml"`** spezifiziert. #
Die Anzahl der zu extrahierenden Faktoren wird durch **`nfactors = 5`** festgelegt, was bedeutet, dass fünf Faktoren extrahiert werden sollen.
**`residuals = TRUE`** berechnet die Residuen der Modellanpassung, um die Unterschiede zwischen den beobachteten und geschätzten Korrelationsmatrizen zu überprüfen. **`SMC = TRUE`** verwendet die quadratischen multiplen Korrelationen (Squared Multiple Correlations) zur Schätzung der Kommunalitäten.
Nachdem die Faktorenanalyse ohne Rotation durchgeführt wurde, wird die **Kaiser-Normalisierung** mit der **Oblimin-Rotation** angewendet. Die **`kaiser()`**-Funktion rotiert die extrahierten Faktoren mit der **Oblimin-Rotation**.

Die Methode zur Anwendung der Oblimin-Rotation wird durch **`rotate = "oblimin"`** festgelegt. Schließlich wird **`fa.sort(fit3)`** verwendet, um die Ergebnisse der Rotation zu sortieren und die Interpretation der Faktoren und ihrer Ladungen zu erleichtern.

---
